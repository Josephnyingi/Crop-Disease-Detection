{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josephnyingi/Crop-Disease-Detection/blob/main/Crop_leaf_disease_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BLM6UPLQ_um"
      },
      "source": [
        "# **Plant Disease Classification**\n",
        "\n",
        "---\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*FswlF4lZPQ4kT_gkybacZw.jpeg)\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Getting affected by a disease is very common in plants due to various factors such as fertilizers, cultural practices followed, environmental conditions, etc. These diseases hurt agricultural yield and eventually the economy based on it. \n",
        "\n",
        "Any technique or method to overcome this problem and getting a warning before the plants are infected would aid farmers to efficiently cultivate crops or plants, both qualitatively and quantitatively. Thus, disease detection in plants plays a very important role in agriculture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR1Te6891aYO"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLk_Uk5N1Xel",
        "outputId": "bed25b06-afba-41cf-ca26-6c47504a9d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown) (4.9.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,544 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,017 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,021 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,136 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,313 kB]\n",
            "Fetched 10.4 MB in 6s (1,695 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package fil\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.24.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.15.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "#Library\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install gdown\n",
        "!apt-get update && apt-get install fil\n",
        "!pip install keras_preprocessing\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Chojawz3jw"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyRk7kzTzOpO"
      },
      "source": [
        "Initializing a few parameters required for the image dataset preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JRRSXKqqed2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c553c3bf-2a0f-4aab-974a-b804d1de8ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.25.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX2j-WO0hHRr"
      },
      "source": [
        "Upload the kaggle.json file to your Google Colab environment using the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Sbwmqp8xhQ3J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "bd871de0-cbb2-4642-999b-dbc69c3b2028"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7fd253c4-25a5-4140-ad6c-ee00ac33e261\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7fd253c4-25a5-4140-ad6c-ee00ac33e261\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"josephnyingi\",\"key\":\"28b0e2f553acb578399142e0a2a9bf9b\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHnd7vGYiayq"
      },
      "source": [
        "After uploading the kaggle.json file, you need to move it to the /root/.kaggle directory in your Google Colab environment using the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0yo9q9H9ic11"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNeCJhEbitkA"
      },
      "source": [
        "Finally, make sure the kaggle.json file has the right permissions using the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H--wpfDaiwbX"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T-CqNgYjFyh"
      },
      "source": [
        "Kaggle API client in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TLV1Nt5Neqec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0101c084-6087-41b5-c21f-cdc16300ee26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading plantvillage.zip to /content\n",
            " 99% 809M/818M [00:06<00:00, 150MB/s]\n",
            "100% 818M/818M [00:06<00:00, 139MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d mohitsingh1804/plantvillage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgFFlaC5letl"
      },
      "source": [
        "Unzip the the Plantvillage.Zip folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y9-ew36kmeIY"
      },
      "outputs": [],
      "source": [
        "# Unzip the downloaded file\n",
        "!unzip -q /content/plantvillage.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing a few parameters required for the image dataset preprocessing."
      ],
      "metadata": {
        "id": "UOVR7Lc34oME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v5eHthEdIBsr"
      },
      "outputs": [],
      "source": [
        "# Dimension of resized image\n",
        "DEFAULT_IMAGE_SIZE = tuple((256, 256))\n",
        "\n",
        "# Number of images used to train the model\n",
        "N_IMAGES = 100\n",
        "\n",
        "# Path to the dataset folder\n",
        "root_dir = '/content/PlantVillage'\n",
        "\n",
        "train_dir = os.path.join(root_dir, 'train')\n",
        "val_dir = os.path.join(root_dir, 'val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyFL0dIxz-zk"
      },
      "source": [
        "We use the function `convert_image_to_array` to resize an image to the size `DEFAULT_IMAGE_SIZE` we defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WnCNrzC539qT"
      },
      "outputs": [],
      "source": [
        "def convert_image_to_array(image_dir):\n",
        "    try:\n",
        "        image = cv2.imread(image_dir)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, DEFAULT_IMAGE_SIZE)   \n",
        "            return img_to_array(image)\n",
        "        else:\n",
        "            return np.array([])\n",
        "    except Exception as e:\n",
        "        print(f\"Error : {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7uyGLRR1RaB"
      },
      "source": [
        "Here, we load the training data images by traversing through all the folders and converting all the images and labels into separate lists respectively. And get number of images for training.\n",
        "\n",
        "*NOTE: We use a small portion of the entire dataset due to the computing limitations. Tweak `N_IMAGES` to include entire dataset.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MPaECpW13__5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644f4246-5715-4a7a-ae31-b9e4149c57e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading images ...\n",
            "[INFO] Processing Strawberry___healthy ...\n",
            "[INFO] Processing Strawberry___Leaf_scorch ...\n",
            "[INFO] Processing Tomato___Bacterial_spot ...\n",
            "[INFO] Processing Tomato___healthy ...\n",
            "[INFO] Processing Apple___Apple_scab ...\n",
            "[INFO] Processing Pepper,_bell___healthy ...\n",
            "[INFO] Processing Tomato___Septoria_leaf_spot ...\n",
            "[INFO] Processing Tomato___Tomato_Yellow_Leaf_Curl_Virus ...\n",
            "[INFO] Processing Apple___Black_rot ...\n",
            "[INFO] Processing Pepper,_bell___Bacterial_spot ...\n",
            "[INFO] Processing Apple___healthy ...\n",
            "[INFO] Processing Potato___Early_blight ...\n",
            "[INFO] Processing Corn_(maize)___Northern_Leaf_Blight ...\n",
            "[INFO] Processing Raspberry___healthy ...\n",
            "[INFO] Processing Cherry_(including_sour)___Powdery_mildew ...\n",
            "[INFO] Processing Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot ...\n",
            "[INFO] Processing Grape___healthy ...\n",
            "[INFO] Processing Peach___Bacterial_spot ...\n",
            "[INFO] Processing Corn_(maize)___Common_rust_ ...\n",
            "[INFO] Processing Tomato___Spider_mites Two-spotted_spider_mite ...\n",
            "[INFO] Processing Orange___Haunglongbing_(Citrus_greening) ...\n",
            "[INFO] Processing Apple___Cedar_apple_rust ...\n",
            "[INFO] Processing Soybean___healthy ...\n",
            "[INFO] Processing Tomato___Target_Spot ...\n",
            "[INFO] Processing Potato___Late_blight ...\n",
            "[INFO] Processing Peach___healthy ...\n",
            "[INFO] Processing Potato___healthy ...\n",
            "[INFO] Processing Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ...\n",
            "[INFO] Processing Cherry_(including_sour)___healthy ...\n",
            "[INFO] Processing Tomato___Leaf_Mold ...\n",
            "[INFO] Processing Corn_(maize)___healthy ...\n",
            "[INFO] Processing Grape___Esca_(Black_Measles) ...\n",
            "[INFO] Processing Blueberry___healthy ...\n",
            "[INFO] Processing Tomato___Early_blight ...\n",
            "[INFO] Processing Tomato___Tomato_mosaic_virus ...\n",
            "[INFO] Processing Squash___Powdery_mildew ...\n",
            "[INFO] Processing Grape___Black_rot ...\n",
            "[INFO] Processing Tomato___Late_blight ...\n",
            "[INFO] Image loading completed\n",
            "\n",
            "Total number of images: 3800\n"
          ]
        }
      ],
      "source": [
        "image_list, label_list = [], []\n",
        "\n",
        "try:\n",
        "    print(\"[INFO] Loading images ...\")\n",
        "    plant_disease_folder_list = listdir(train_dir)\n",
        "\n",
        "    for plant_disease_folder in plant_disease_folder_list:\n",
        "        print(f\"[INFO] Processing {plant_disease_folder} ...\")\n",
        "        plant_disease_image_list = listdir(f\"{train_dir}/{plant_disease_folder}/\")\n",
        "\n",
        "        for image in plant_disease_image_list[:N_IMAGES]:\n",
        "            image_directory = f\"{train_dir}/{plant_disease_folder}/{image}\"\n",
        "            if image_directory.endswith(\".jpg\")==True or image_directory.endswith(\".JPG\")==True:\n",
        "                image_list.append(convert_image_to_array(image_directory))\n",
        "                label_list.append(plant_disease_folder)\n",
        "\n",
        "    print(\"[INFO] Image loading completed\")  \n",
        "except Exception as e:\n",
        "    print(f\"Error : {e}\")\n",
        "\n",
        "# Transform the loaded training image data into numpy array\n",
        "np_image_list = np.array(image_list, dtype=np.float16) / 225.0\n",
        "print()\n",
        "\n",
        "# Check the number of images loaded for training\n",
        "image_len = len(image_list)\n",
        "print(f\"Total number of images: {image_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOzIsDNP3TWy"
      },
      "source": [
        "Examine the labels/classes in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jq5HO2SuILNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d51012d-4fb6-4b16-d0c1-f5189a59cbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of classes:  38\n"
          ]
        }
      ],
      "source": [
        "label_binarizer = LabelBinarizer()\n",
        "image_labels = label_binarizer.fit_transform(label_list)\n",
        "\n",
        "pickle.dump(label_binarizer,open('plant_disease_label_transform.pkl', 'wb'))\n",
        "n_classes = len(label_binarizer.classes_)\n",
        "\n",
        "print(\"Total number of classes: \", n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siSjS-jG77FH"
      },
      "source": [
        "# Augment and Split Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G44Cdatx8VVN"
      },
      "source": [
        "Using `ImageDataGenerator` to augment data by performing various operations on the training images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pbIYhnSFJkjp"
      },
      "outputs": [],
      "source": [
        "augment = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
        "                             height_shift_range=0.1, shear_range=0.2, \n",
        "                             zoom_range=0.2, horizontal_flip=True, \n",
        "                             fill_mode=\"nearest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaoNWFDd8swl"
      },
      "source": [
        "Splitting the data into training and test sets for validation purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0Ls-TztoQlwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c11cf45-da05-4374-8e31-cf82cf7bfe27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Splitting data to train and test...\n"
          ]
        }
      ],
      "source": [
        "print(\"[INFO] Splitting data to train and test...\")\n",
        "x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4VNmekW4oEN"
      },
      "source": [
        "# Build Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QUHIVIcg6qI"
      },
      "source": [
        "Defining the hyperparameters of the plant disease classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BLZkDigJ9q4X"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 25\n",
        "STEPS = 100\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "WIDTH = 256\n",
        "HEIGHT = 256\n",
        "DEPTH = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAlqxrJzha2n"
      },
      "source": [
        "Creating a sequential model and adding Convolutional, Normalization, Pooling, Dropout and Activation layers at the appropriate positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J_QFYYx8OVOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126c5612-ef7d-4ff5-b3a0-460d62b3d412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 256, 256, 32)      896       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 256, 256, 32)      0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256, 256, 32)     128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 85, 85, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 85, 85, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 85, 85, 64)        18496     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 85, 85, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 85, 85, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 85, 85, 64)        36928     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 85, 85, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 85, 85, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 42, 42, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 42, 42, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 42, 42, 128)       73856     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 42, 42, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 42, 42, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 42, 42, 128)       147584    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 42, 42, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 42, 42, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 21, 21, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 21, 21, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 56448)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              57803776  \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 38)                38950     \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 38)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 58,126,246\n",
            "Trainable params: 58,123,366\n",
            "Non-trainable params: 2,880\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "inputShape = (HEIGHT, WIDTH, DEPTH)\n",
        "chanDim = -1\n",
        "\n",
        "if K.image_data_format() == \"channels_first\":\n",
        "    inputShape = (DEPTH, HEIGHT, WIDTH)\n",
        "    chanDim = 1\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(n_classes))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViKAhHPN5VMM"
      },
      "source": [
        "# Train Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syZMhHG-iFlT"
      },
      "source": [
        "We initialize Adam optimizer with learning rate and decay parameters. \n",
        "\n",
        "Also, we choose the type of loss and metrics for the model and compile it for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFRGnmHiO0Wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414edd40-3a2c-41e3-afa5-b805b2c6266f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training network...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "<ipython-input-19-6d88faf22259>:9: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(augment.flow(x_train, y_train, batch_size=BATCH_SIZE),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "17/95 [====>.........................] - ETA: 12:56 - loss: 0.8492 - accuracy: 0.0607"
          ]
        }
      ],
      "source": [
        "# Initialize optimizer\n",
        "opt = Adam(lr=LR, decay=LR / EPOCHS)\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model\n",
        "print(\"[INFO] Training network...\")\n",
        "history = model.fit_generator(augment.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "                              validation_data=(x_test, y_test),\n",
        "                              steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
        "                              epochs=EPOCHS, \n",
        "                              verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6gp-hPe9Rra"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75wjEVZ-9Yab"
      },
      "source": [
        "Comparing the accuracy and loss by plotting the graph for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68DC4UqXTH4r"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Train and validation accuracy\n",
        "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
        "plt.title('Training and Validation accurarcy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Train and validation loss\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXO9rDqtNlDN"
      },
      "source": [
        "Evaluating model accuracy by using the `evaluate` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlvFAyeuRQPK"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] Calculating model accuracy\")\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02FiJNWTN5J0"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m84g3UW4RW0j"
      },
      "outputs": [],
      "source": [
        "# Dump pickle file of the model\n",
        "print(\"[INFO] Saving model...\")\n",
        "pickle.dump(model,open('plant_disease_classification_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkzOGExsRe3I"
      },
      "outputs": [],
      "source": [
        "# Dump pickle file of the labels\n",
        "print(\"[INFO] Saving label transform...\")\n",
        "filename = 'plant_disease_label_transform.pkl'\n",
        "image_labels = pickle.load(open(filename, 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvKwCACObpp"
      },
      "source": [
        "# Test Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKZcE-RmiuvH"
      },
      "source": [
        "We write the following `predict_disease` function to predict the class or disease of a plant image. \n",
        "\n",
        "We just need to provide the complete path to the image and it displays the image along with its prediction class or plant disease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxhKVIETOuah"
      },
      "outputs": [],
      "source": [
        "def predict_disease(image_path):\n",
        "    image_array = convert_image_to_array(image_path)\n",
        "    np_image = np.array(image_array, dtype=np.float16) / 225.0\n",
        "    np_image = np.expand_dims(np_image,0)\n",
        "    plt.imshow(plt.imread(image_path))\n",
        "    result = model.predict_classes(np_image)\n",
        "    print((image_labels.classes_[result][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93kbeqlSjCb-"
      },
      "source": [
        "For testing purposes, we randomly choose images from the dataset and try predicting class or disease of the plant image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2Eq3ybER2dX"
      },
      "outputs": [],
      "source": [
        "predict_disease('/content/PlantVillage/val/Blueberry___healthy/008c85d0-a954-4127-bd26-861dc8a1e6ff___RS_HL 2431.JPG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx7zkdJWlLAz"
      },
      "outputs": [],
      "source": [
        "predict_disease('/content/PlantVillage/val/Potato___Early_blight/03b0d3c1-b5b0-48f4-98aa-f8904670290f___RS_Early.B 7051.JPG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJb3gtAWlgJW"
      },
      "outputs": [],
      "source": [
        "predict_disease('/content/PlantVillage/val/Tomato___Target_Spot/1006b3dd-22d8-41b8-b83d-08bf189fcdaa___Com.G_TgS_FL 8118.JPG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SVSwms_ln3B"
      },
      "outputs": [],
      "source": [
        "predict_disease('/content/PlantVillage/val/Orange___Haunglongbing_(Citrus_greening)/02459e0c-a189-4dc9-a0dc-0548e36d0efb___CREC_HLB 5714.JPG')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EMeS2AZPqwf"
      },
      "source": [
        "# Reuse Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05wL4YREP4tp"
      },
      "source": [
        "We download the trained model and label transform saved in the Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK-FS4h-l7UW"
      },
      "outputs": [],
      "source": [
        "# Download the trained model file based on its file ID.\n",
        "file_id = '1E5jNzpM__7z67GRl1cbhHK71yKcPa8wl'\n",
        "!gdown https://drive.google.com/uc?id={file_id}\n",
        "\n",
        "# Download the labels file based on its file ID.\n",
        "file_id = '1WsgEd3TG33Vj_9AAAT_WfJe_AqsuC9uu'\n",
        "!gdown https://drive.google.com/uc?id={file_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqTsg-ZskDvE"
      },
      "source": [
        "Importing necessary libraries and modules required to build the classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuyQWhlqj-jz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5XLJbmKQiJE"
      },
      "source": [
        "Load the trained model and its labels for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_BpNjfQmgHI"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "filename = 'plant_disease_classification_model.pkl'\n",
        "model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "# Load labels\n",
        "filename = 'plant_disease_label_transform.pkl'\n",
        "image_labels = pickle.load(open(filename, 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KFhgotClJv8"
      },
      "source": [
        "We use the `convert_image_to_array` function to resize an image and `predict_disease` function to predict the class or disease of a plant image.\n",
        "\n",
        "We just need to provide the complete path to the image and it displays the image along with its prediction class or plant disease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WTErOo1jigq"
      },
      "outputs": [],
      "source": [
        "# Dimension of resized image\n",
        "DEFAULT_IMAGE_SIZE = tuple((256, 256))\n",
        "\n",
        "def convert_image_to_array(image_dir):\n",
        "    try:\n",
        "        image = cv2.imread(image_dir)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, DEFAULT_IMAGE_SIZE)   \n",
        "            return img_to_array(image)\n",
        "        else:\n",
        "            return np.array([])\n",
        "    except Exception as e:\n",
        "        print(f\"Error : {e}\")\n",
        "        return None\n",
        "\n",
        "def predict_disease(image_path):\n",
        "    image_array = convert_image_to_array(image_path)\n",
        "    np_image = np.array(image_array, dtype=np.float16) / 225.0\n",
        "    np_image = np.expand_dims(np_image,0)\n",
        "    plt.imshow(plt.imread(image_path))\n",
        "    result = model.predict_classes(np_image)\n",
        "    print((image_labels.classes_[result][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6BO6sPyQx-p"
      },
      "source": [
        "Predict disease of any plant image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXnqW4w3mkLA"
      },
      "outputs": [],
      "source": [
        "predict_disease('/content/PlantVillage/val/Corn_(maize)___Northern_Leaf_Blight/028159fc-995e-455a-8d60-6d377580a898___RS_NLB 4023.JPG')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}